This is an implementation of a least-frequently-used (LFU) cache that has `O(1)` time complexity for both the `put` and `get` operations.  The cache stores key/value pairs until it reaches the capacity with which it was initialized.  Any additional entries into the cache evicts the least frequently used entry, where a use is defined as either updating a value (with another call to put) or retrieving a value (with a call to get).

Achieving `O(log n)` insertion complexity is fairly intuitive with the usage of a min-heap, i.e. maintain a heap of tuples of the form `(frequency, key, value)` and pop off items from the top of the heap as capacity is reached. However, lookups and updates are tricky, as we have to then find the correct tuple and update its frequency.  Even if we map the keys and tuples using a dictionary (introducing some other problems as tuples are immutable in Python), heapifying the heap takes `O(n)` time.

In order to achieve constant time complexity, we abandon the heap and use doubly linked lists instead.  We use two types of linked list nodes, both intended to be private to the LFUCache class.  Item nodes store the actual key/value pairs along with a frequency counter that is incremented every time the node is accessed.  Frequency nodes store item nodes at the same frequency.  These latter nodes are created and deleted by the cache as needed.  Lastly, we need a dictionary to map between key values and their respective item nodes, in order to enable constant lookup time.  